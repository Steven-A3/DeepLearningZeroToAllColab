{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_8_PlayingCartPoleRandom",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Steven-A3/DeepLearningZeroToAllColab/blob/master/RL_8_PlayingCartPoleRandom.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odNaDE1zyrL2",
        "colab_type": "text"
      },
      "source": [
        "# install dependancies, takes around 45 seconds\n",
        "\n",
        "Rendering Dependancies\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-AxnvAVyzQQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8A-1LTSH88EE",
        "colab_type": "text"
      },
      "source": [
        "CartPole Dependancies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCelFzWY9MBI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APXSx7hg19TH",
        "colab_type": "text"
      },
      "source": [
        "# Imports and Helper functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdb2JwZy4jGj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQEtc28G4niA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9UWeToN4r7D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3BGbWOu179M",
        "colab_type": "text"
      },
      "source": [
        "# CartPole!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGEFMfDOzLen",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = wrap_env(gym.make(\"CartPole-v0\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BmIlXhe9Q89",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#check out the CartPole action space!\n",
        "print(env.action_space)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nj5sjsk15IT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env.reset()\n",
        "random_episodes = 0\n",
        "reward_sum = 0\n",
        "while random_episodes < 10:\n",
        "    env.render()\n",
        "    action = env.action_space.sample()\n",
        "    observation, reward, done, _ = env.step(action)\n",
        "    print(observation, reward, done)\n",
        "    reward_sum += reward\n",
        "    if done:\n",
        "        random_episodes += 1\n",
        "        print(\"Reward for this episode was:\", reward_sum)\n",
        "        reward_sum = 0\n",
        "        env.reset()\n",
        "        \n",
        "show_video()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uynQrxJyj3RZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Constants defining our neural network\n",
        "learning_rate = 1e-1\n",
        "input_size = env.observation_space.shape[0]\n",
        "output_size = env.action_space.n\n",
        "\n",
        "X = tf.placeholder(tf.float32, [None, input_size], name=\"input_x\")\n",
        "\n",
        "# First layer of weights\n",
        "W1 = tf.get_variable(\"W1\", shape=[input_size, output_size],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "Qpred = tf.matmul(X, W1)\n",
        "\n",
        "# We need to define the parts of the network needed for learning a policy\n",
        "Y = tf.placeholder(shape=[None, output_size], dtype=tf.float32)\n",
        "\n",
        "# Loss function\n",
        "loss = tf.reduce_sum(tf.square(Y - Qpred))\n",
        "# Learning\n",
        "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
        "\n",
        "# Values for q learning\n",
        "max_episodes = 5000\n",
        "dis = 0.9\n",
        "step_history = []\n",
        "\n",
        "\n",
        "# Setting up our environment\n",
        "init = tf.global_variables_initializer()\n",
        "sess = tf.Session()\n",
        "sess.run(init)\n",
        "\n",
        "for episode in range(max_episodes):\n",
        "    e = 1. / ((episode / 10) + 1)\n",
        "    step_count = 0\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "\n",
        "    # The Q-Network training\n",
        "    while not done:\n",
        "        step_count += 1\n",
        "        x = np.reshape(state, [1, input_size])\n",
        "        # Choose an action by greedily (with e chance of random action) from\n",
        "        # the Q-network\n",
        "        Q = sess.run(Qpred, feed_dict={X: x})\n",
        "        if np.random.rand(1) < e:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            action = np.argmax(Q)\n",
        "\n",
        "        # Get new state and reward from environment\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        if done:\n",
        "            Q[0, action] = -100\n",
        "        else:\n",
        "            x_next = np.reshape(next_state, [1, input_size])\n",
        "            # Obtain the Q' values by feeding the new state through our network\n",
        "            Q_next = sess.run(Qpred, feed_dict={X: x_next})\n",
        "            Q[0, action] = reward + dis * np.max(Q_next)\n",
        "\n",
        "        # Train our network using target and predicted Q values on each episode\n",
        "        sess.run(train, feed_dict={X: x, Y: Q})\n",
        "        state = next_state\n",
        "\n",
        "    step_history.append(step_count)\n",
        "    print(\"Episode: {}  steps: {}\".format(episode, step_count))\n",
        "    # If last 10's avg steps are 500, it's good enough\n",
        "    if len(step_history) > 10 and np.mean(step_history[-10:]) > 500:\n",
        "        break\n",
        "\n",
        "# See our trained network in action\n",
        "observation = env.reset()\n",
        "reward_sum = 0\n",
        "while True:\n",
        "    env.render()\n",
        "\n",
        "    x = np.reshape(observation, [1, input_size])\n",
        "    Q = sess.run(Qpred, feed_dict={X: x})\n",
        "    action = np.argmax(Q)\n",
        "\n",
        "    observation, reward, done, _ = env.step(action)\n",
        "    reward_sum += reward\n",
        "    if done:\n",
        "        print(\"Total score: {}\".format(reward_sum))\n",
        "        break\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpJTWRkcn_VL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQN:\n",
        "\n",
        "    def __init__(self, session: tf.Session, input_size: int, output_size: int, name: str=\"main\") -> None:\n",
        "        \"\"\"DQN Agent can\n",
        "\n",
        "        1) Build network\n",
        "        2) Predict Q_value given state\n",
        "        3) Train parameters\n",
        "\n",
        "        Args:\n",
        "            session (tf.Session): Tensorflow session\n",
        "            input_size (int): Input dimension\n",
        "            output_size (int): Number of discrete actions\n",
        "            name (str, optional): TF Graph will be built under this name scope\n",
        "        \"\"\"\n",
        "        self.session = session\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.net_name = name\n",
        "\n",
        "        self._build_network()\n",
        "\n",
        "    def _build_network(self, h_size=16, l_rate=0.001) -> None:\n",
        "        \"\"\"DQN Network architecture (simple MLP)\n",
        "\n",
        "        Args:\n",
        "            h_size (int, optional): Hidden layer dimension\n",
        "            l_rate (float, optional): Learning rate\n",
        "        \"\"\"\n",
        "        with tf.variable_scope(self.net_name):\n",
        "            self._X = tf.placeholder(tf.float32, [None, self.input_size], name=\"input_x\")\n",
        "            net = self._X\n",
        "\n",
        "            net = tf.layers.dense(net, h_size, activation=tf.nn.relu)\n",
        "            net = tf.layers.dense(net, self.output_size)\n",
        "            self._Qpred = net\n",
        "\n",
        "            self._Y = tf.placeholder(tf.float32, shape=[None, self.output_size])\n",
        "            self._loss = tf.losses.mean_squared_error(self._Y, self._Qpred)\n",
        "\n",
        "            optimizer = tf.train.AdamOptimizer(learning_rate=l_rate)\n",
        "            self._train = optimizer.minimize(self._loss)\n",
        "\n",
        "    def predict(self, state: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Returns Q(s, a)\n",
        "\n",
        "        Args:\n",
        "            state (np.ndarray): State array, shape (n, input_dim)\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Q value array, shape (n, output_dim)\n",
        "        \"\"\"\n",
        "        x = np.reshape(state, [-1, self.input_size])\n",
        "        return self.session.run(self._Qpred, feed_dict={self._X: x})\n",
        "\n",
        "    def update(self, x_stack: np.ndarray, y_stack: np.ndarray) -> list:\n",
        "        \"\"\"Performs updates on given X and y and returns a result\n",
        "\n",
        "        Args:\n",
        "            x_stack (np.ndarray): State array, shape (n, input_dim)\n",
        "            y_stack (np.ndarray): Target Q array, shape (n, output_dim)\n",
        "\n",
        "        Returns:\n",
        "            list: First element is loss, second element is a result from train step\n",
        "        \"\"\"\n",
        "        feed = {\n",
        "            self._X: x_stack,\n",
        "            self._Y: y_stack\n",
        "        }\n",
        "        return self.session.run([self._loss, self._train], feed)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6UjvqHflAzh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_SIZE = env.observation_space.shape[0]\n",
        "OUTPUT_SIZE = env.action_space.n\n",
        "\n",
        "DISCOUNT_RATE = 0.99\n",
        "REPLAY_MEMORY = 50000\n",
        "MAX_EPISODE = 5000\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# minimum epsilon for epsilon greedy\n",
        "MIN_E = 0.0\n",
        "# epsilon will be `MIN_E` at `EPSILON_DECAYING_EPISODE`\n",
        "EPSILON_DECAYING_EPISODE = MAX_EPISODE * 0.01\n",
        "\n",
        "\n",
        "def bot_play(mainDQN: DQN) -> None:\n",
        "    \"\"\"Runs a single episode with rendering and prints a reward\n",
        "\n",
        "    Args:\n",
        "        mainDQN (dqn.DQN): DQN Agent\n",
        "    \"\"\"\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "\n",
        "    while True:\n",
        "        env.render()\n",
        "        action = np.argmax(mainDQN.predict(state))\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "        if done:\n",
        "            print(\"Total score: {}\".format(total_reward))\n",
        "            break\n",
        "\n",
        "\n",
        "def train_minibatch(DQN: DQN, train_batch: list) -> float:\n",
        "    \"\"\"Prepare X_batch, y_batch and train them\n",
        "\n",
        "    Recall our loss function is\n",
        "        target = reward + discount * max Q(s',a)\n",
        "                 or reward if done early\n",
        "\n",
        "        Loss function: [target - Q(s, a)]^2\n",
        "\n",
        "    Hence,\n",
        "\n",
        "        X_batch is a state list\n",
        "        y_batch is reward + discount * max Q\n",
        "                   or reward if terminated early\n",
        "\n",
        "    Args:\n",
        "        DQN (dqn.DQN): DQN Agent to train & run\n",
        "        train_batch (list): Minibatch of Replay memory\n",
        "            Eeach element is a tuple of (s, a, r, s', done)\n",
        "\n",
        "    Returns:\n",
        "        loss: Returns a loss\n",
        "\n",
        "    \"\"\"\n",
        "    state_array = np.vstack([x[0] for x in train_batch])\n",
        "    action_array = np.array([x[1] for x in train_batch])\n",
        "    reward_array = np.array([x[2] for x in train_batch])\n",
        "    next_state_array = np.vstack([x[3] for x in train_batch])\n",
        "    done_array = np.array([x[4] for x in train_batch])\n",
        "\n",
        "    X_batch = state_array\n",
        "    y_batch = DQN.predict(state_array)\n",
        "\n",
        "    Q_target = reward_array + DISCOUNT_RATE * np.max(DQN.predict(next_state_array), axis=1) * ~done_array\n",
        "    y_batch[np.arange(len(X_batch)), action_array] = Q_target\n",
        "\n",
        "    # Train our network using target and predicted Q values on each episode\n",
        "    loss, _ = DQN.update(X_batch, y_batch)\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def annealing_epsilon(episode: int, min_e: float, max_e: float, target_episode: int) -> float:\n",
        "    \"\"\"Return an linearly annealed epsilon\n",
        "\n",
        "    Epsilon will decrease over time until it reaches `target_episode`\n",
        "\n",
        "         (epsilon)\n",
        "             |\n",
        "    max_e ---|\\\n",
        "             | \\\n",
        "             |  \\\n",
        "             |   \\\n",
        "    min_e ---|____\\_______________(episode)\n",
        "                  |\n",
        "                 target_episode\n",
        "\n",
        "     slope = (min_e - max_e) / (target_episode)\n",
        "     intercept = max_e\n",
        "\n",
        "     e = slope * episode + intercept\n",
        "\n",
        "    Args:\n",
        "        episode (int): Current episode\n",
        "        min_e (float): Minimum epsilon\n",
        "        max_e (float): Maximum epsilon\n",
        "        target_episode (int): epsilon becomes the `min_e` at `target_episode`\n",
        "\n",
        "    Returns:\n",
        "        float: epsilon between `min_e` and `max_e`\n",
        "    \"\"\"\n",
        "\n",
        "    slope = (min_e - max_e) / (target_episode)\n",
        "    intercept = max_e\n",
        "\n",
        "    return max(min_e, slope * episode + intercept)\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqpBan_xnY_1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import deque\n",
        "\n",
        "# store the previous observations in replay memory\n",
        "replay_buffer = deque(maxlen=REPLAY_MEMORY)\n",
        "last_100_game_reward = deque(maxlen=100)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    mainDQN = DQN(sess, INPUT_SIZE, OUTPUT_SIZE)\n",
        "    init = tf.global_variables_initializer()\n",
        "    sess.run(init)\n",
        "\n",
        "    for episode in range(MAX_EPISODE):\n",
        "        e = annealing_epsilon(episode, MIN_E, 1.0, EPSILON_DECAYING_EPISODE)\n",
        "        done = False\n",
        "        state = env.reset()\n",
        "\n",
        "        step_count = 0\n",
        "        while not done:\n",
        "\n",
        "            if np.random.rand() < e:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                action = np.argmax(mainDQN.predict(state))\n",
        "\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            if done:\n",
        "                reward = -1\n",
        "\n",
        "            replay_buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "            state = next_state\n",
        "            step_count += 1\n",
        "\n",
        "            if len(replay_buffer) > BATCH_SIZE:\n",
        "                minibatch = random.sample(replay_buffer, BATCH_SIZE)\n",
        "                train_minibatch(mainDQN, minibatch)\n",
        "\n",
        "        print(\"[Episode {:>5}]  steps: {:>5} e: {:>5.2f}\".format(episode, step_count, e))\n",
        "\n",
        "        # CartPole-v0 Game Clear Logic\n",
        "        last_100_game_reward.append(step_count)\n",
        "        if len(last_100_game_reward) == last_100_game_reward.maxlen:\n",
        "            avg_reward = np.mean(last_100_game_reward)\n",
        "            if avg_reward > 199.0:\n",
        "                print(\"Game Cleared within {} episodes with avg reward {}\".format(episode, avg_reward))\n",
        "                break\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}