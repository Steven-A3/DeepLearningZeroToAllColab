{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_9_PlayingCartPole DQN",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Steven-A3/DeepLearningZeroToAllColab/blob/master/RL_9_PlayingCartPole_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odNaDE1zyrL2",
        "colab_type": "text"
      },
      "source": [
        "# install dependancies, takes around 45 seconds\n",
        "\n",
        "Rendering Dependancies\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-AxnvAVyzQQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8A-1LTSH88EE",
        "colab_type": "text"
      },
      "source": [
        "CartPole Dependancies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCelFzWY9MBI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APXSx7hg19TH",
        "colab_type": "text"
      },
      "source": [
        "# Imports and Helper functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdb2JwZy4jGj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQEtc28G4niA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9UWeToN4r7D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3BGbWOu179M",
        "colab_type": "text"
      },
      "source": [
        "# CartPole!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGEFMfDOzLen",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gym.envs.registration import register\n",
        "\n",
        "register(\n",
        "    id='CartPole-v10',\n",
        "    entry_point='gym.envs.classic_control:CartPoleEnv',\n",
        "    max_episode_steps=10000,\n",
        "    reward_threshold=195.0,\n",
        ")\n",
        "\n",
        "env = wrap_env(gym.make(\"CartPole-v10\"))\n",
        "env = gym.wrappers.Monitor(env, directory=\"gym-results/\", force=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BmIlXhe9Q89",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#check out the CartPole action space!\n",
        "print(env.action_space)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpJTWRkcn_VL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import deque\n",
        "from typing import List\n",
        "\n",
        "class DQN:\n",
        "\n",
        "    def __init__(self, session: tf.Session, input_size: int, output_size: int, name: str=\"main\") -> None:\n",
        "        \"\"\"DQN Agent can\n",
        "\n",
        "        1) Build network\n",
        "        2) Predict Q_value given state\n",
        "        3) Train parameters\n",
        "\n",
        "        Args:\n",
        "            session (tf.Session): Tensorflow session\n",
        "            input_size (int): Input dimension\n",
        "            output_size (int): Number of discrete actions\n",
        "            name (str, optional): TF Graph will be built under this name scope\n",
        "        \"\"\"\n",
        "        self.session = session\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.net_name = name\n",
        "\n",
        "        self._build_network()\n",
        "\n",
        "    def _build_network(self, h_size=16, l_rate=0.001) -> None:\n",
        "        \"\"\"DQN Network architecture (simple MLP)\n",
        "\n",
        "        Args:\n",
        "            h_size (int, optional): Hidden layer dimension\n",
        "            l_rate (float, optional): Learning rate\n",
        "        \"\"\"\n",
        "        with tf.variable_scope(self.net_name):\n",
        "            self._X = tf.placeholder(tf.float32, [None, self.input_size], name=\"input_x\")\n",
        "            net = self._X\n",
        "\n",
        "            net = tf.layers.dense(net, h_size, activation=tf.nn.relu)\n",
        "            net = tf.layers.dense(net, self.output_size)\n",
        "            self._Qpred = net\n",
        "\n",
        "            self._Y = tf.placeholder(tf.float32, shape=[None, self.output_size])\n",
        "            self._loss = tf.losses.mean_squared_error(self._Y, self._Qpred)\n",
        "\n",
        "            optimizer = tf.train.AdamOptimizer(learning_rate=l_rate)\n",
        "            self._train = optimizer.minimize(self._loss)\n",
        "\n",
        "    def predict(self, state: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Returns Q(s, a)\n",
        "\n",
        "        Args:\n",
        "            state (np.ndarray): State array, shape (n, input_dim)\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Q value array, shape (n, output_dim)\n",
        "        \"\"\"\n",
        "        x = np.reshape(state, [-1, self.input_size])\n",
        "        return self.session.run(self._Qpred, feed_dict={self._X: x})\n",
        "\n",
        "    def update(self, x_stack: np.ndarray, y_stack: np.ndarray) -> list:\n",
        "        \"\"\"Performs updates on given X and y and returns a result\n",
        "\n",
        "        Args:\n",
        "            x_stack (np.ndarray): State array, shape (n, input_dim)\n",
        "            y_stack (np.ndarray): Target Q array, shape (n, output_dim)\n",
        "\n",
        "        Returns:\n",
        "            list: First element is loss, second element is a result from train step\n",
        "        \"\"\"\n",
        "        feed = {\n",
        "            self._X: x_stack,\n",
        "            self._Y: y_stack\n",
        "        }\n",
        "        return self.session.run([self._loss, self._train], feed)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PYDo6gK9yMn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Constants defining our neural network\n",
        "INPUT_SIZE = env.observation_space.shape[0]\n",
        "OUTPUT_SIZE = env.action_space.n\n",
        "\n",
        "DISCOUNT_RATE = 0.99\n",
        "REPLAY_MEMORY = 50000\n",
        "BATCH_SIZE = 64\n",
        "TARGET_UPDATE_FREQUENCY = 5\n",
        "MAX_EPISODES = 5000\n",
        "\n",
        "\n",
        "def replay_train(mainDQN: DQN, targetDQN: DQN, train_batch: list) -> float:\n",
        "    \"\"\"Trains `mainDQN` with target Q values given by `targetDQN`\n",
        "\n",
        "    Args:\n",
        "        mainDQN (dqn.DQN): Main DQN that will be trained\n",
        "        targetDQN (dqn.DQN): Target DQN that will predict Q_target\n",
        "        train_batch (list): Minibatch of replay memory\n",
        "            Each element is (s, a, r, s', done)\n",
        "            [(state, action, reward, next_state, done), ...]\n",
        "\n",
        "    Returns:\n",
        "        float: After updating `mainDQN`, it returns a `loss`\n",
        "    \"\"\"\n",
        "    states = np.vstack([x[0] for x in train_batch])\n",
        "    actions = np.array([x[1] for x in train_batch])\n",
        "    rewards = np.array([x[2] for x in train_batch])\n",
        "    next_states = np.vstack([x[3] for x in train_batch])\n",
        "    done = np.array([x[4] for x in train_batch])\n",
        "\n",
        "    X = states\n",
        "\n",
        "    Q_target = rewards + DISCOUNT_RATE * np.max(targetDQN.predict(next_states), axis=1) * ~done\n",
        "\n",
        "    y = mainDQN.predict(states)\n",
        "    y[np.arange(len(X)), actions] = Q_target\n",
        "\n",
        "    # Train our network using target and predicted Q values on each episode\n",
        "    return mainDQN.update(X, y)\n",
        "\n",
        "\n",
        "def get_copy_var_ops(*, dest_scope_name: str, src_scope_name: str) -> List[tf.Operation]:\n",
        "    \"\"\"Creates TF operations that copy weights from `src_scope` to `dest_scope`\n",
        "\n",
        "    Args:\n",
        "        dest_scope_name (str): Destination weights (copy to)\n",
        "        src_scope_name (str): Source weight (copy from)\n",
        "\n",
        "    Returns:\n",
        "        List[tf.Operation]: Update operations are created and returned\n",
        "    \"\"\"\n",
        "    # Copy variables src_scope to dest_scope\n",
        "    op_holder = []\n",
        "\n",
        "    src_vars = tf.get_collection(\n",
        "        tf.GraphKeys.TRAINABLE_VARIABLES, scope=src_scope_name)\n",
        "    dest_vars = tf.get_collection(\n",
        "        tf.GraphKeys.TRAINABLE_VARIABLES, scope=dest_scope_name)\n",
        "\n",
        "    for src_var, dest_var in zip(src_vars, dest_vars):\n",
        "        op_holder.append(dest_var.assign(src_var.value()))\n",
        "\n",
        "    return op_holder\n",
        "\n",
        "\n",
        "def bot_play(mainDQN: DQN, env: gym.Env) -> None:\n",
        "    \"\"\"Test runs with rendering and prints the total score\n",
        "\n",
        "    Args:\n",
        "        mainDQN (dqn.DQN): DQN agent to run a test\n",
        "        env (gym.Env): Gym Environment\n",
        "    \"\"\"\n",
        "    state = env.reset()\n",
        "    reward_sum = 0\n",
        "\n",
        "    while True:\n",
        "\n",
        "        env.render()\n",
        "        action = np.argmax(mainDQN.predict(state))\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        reward_sum += reward\n",
        "\n",
        "        if done:\n",
        "            print(\"Total score: {}\".format(reward_sum))\n",
        "            break\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7C-6MGLA9zEJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# store the previous observations in replay memory\n",
        "replay_buffer = deque(maxlen=REPLAY_MEMORY)\n",
        "\n",
        "last_100_game_reward = deque(maxlen=100)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    mainDQN = DQN(sess, INPUT_SIZE, OUTPUT_SIZE, name=\"main\")\n",
        "    targetDQN = DQN(sess, INPUT_SIZE, OUTPUT_SIZE, name=\"target\")\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    # initial copy q_net -> target_net\n",
        "    copy_ops = get_copy_var_ops(dest_scope_name=\"target\",\n",
        "                                src_scope_name=\"main\")\n",
        "    sess.run(copy_ops)\n",
        "\n",
        "    for episode in range(MAX_EPISODES):\n",
        "        e = 1. / ((episode / 10) + 1)\n",
        "        done = False\n",
        "        step_count = 0\n",
        "        state = env.reset()\n",
        "\n",
        "        while not done:\n",
        "            if np.random.rand() < e:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                # Choose an action by greedily from the Q-network\n",
        "                action = np.argmax(mainDQN.predict(state))\n",
        "\n",
        "            # Get new state and reward from environment\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            if done:  # Penalty\n",
        "                reward = -1\n",
        "\n",
        "            # Save the experience to our buffer\n",
        "            replay_buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "            if len(replay_buffer) > BATCH_SIZE:\n",
        "                minibatch = random.sample(replay_buffer, BATCH_SIZE)\n",
        "                loss, _ = replay_train(mainDQN, targetDQN, minibatch)\n",
        "\n",
        "            if step_count % TARGET_UPDATE_FREQUENCY == 0:\n",
        "                sess.run(copy_ops)\n",
        "\n",
        "            state = next_state\n",
        "            step_count += 1\n",
        "\n",
        "        print(\"Episode: {}  steps: {}\".format(episode, step_count))\n",
        "\n",
        "        # CartPole-v0 Game Clear Checking Logic\n",
        "        last_100_game_reward.append(step_count)\n",
        "\n",
        "        if len(last_100_game_reward) == last_100_game_reward.maxlen:\n",
        "            avg_reward = np.mean(last_100_game_reward)\n",
        "\n",
        "            if avg_reward > 199:\n",
        "                print(f\"Game Cleared in {episode} episodes with avg reward {avg_reward}\")\n",
        "                break\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}